{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "\n",
    "DATA_PATH=\"/Users/somesh/Desktop/my_gen_ai_folder/Project1/Data\"\n",
    "DB_PATH = \"vectorstore/db_faiss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents loaded: 759\n",
      "Total number of chunks created: 7080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/LANGCHAIN/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#SPTEP1 : Load the PDF files from the directory\n",
    "def load_pdf(directory_path):\n",
    "    loader = DirectoryLoader(\n",
    "        path=directory_path,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls = PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "documents = load_pdf(DATA_PATH)\n",
    "print(f\"Total number of documents loaded: {len(documents)}\") #759\n",
    "\n",
    "\n",
    "#STEP2 : CREATE CHUNKS\n",
    "def create_chunks(documents,chunk_size=500,chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "chunks = create_chunks(documents)\n",
    "print(f\"Total number of chunks created: {len(chunks)}\")  #7080\n",
    "\n",
    "#STEP3 : CREATE EMBEDDING Model\n",
    "def get_embedding_model():\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embedding_model\n",
    "\n",
    "embedding_model = get_embedding_model()\n",
    "\n",
    "#STEP4 : EMBED CHUNKS AND STORE THEM IN VECTOR STORE\n",
    "def create_vector_store(chunks,embedding_model,DB_PATH):\n",
    "    db = FAISS.from_documents(chunks, embedding_model)\n",
    "    db.save_local(DB_PATH)\n",
    "    \n",
    "create_vector_store(chunks, embedding_model, DB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/LANGCHAIN/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#STEP3 : CREATE EMBEDDING Model\n",
    "def get_embedding_model():\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embedding_model\n",
    "\n",
    "embedding_model = get_embedding_model()\n",
    "\n",
    "#STEP5 : LOAD LLM ,retriever and DATA BASE\n",
    "def load_llm():\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        temperature=0.7,\n",
    "        max_output_tokens=50,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def load_vector_store(DB_PATH):\n",
    "    embedding_model = get_embedding_model()\n",
    "    db = FAISS.load_local(DB_PATH, embedding_model,allow_dangerous_deserialization=True)\n",
    "    return db\n",
    "\n",
    "llm = load_llm()\n",
    "db = load_vector_store(DB_PATH)\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m   context_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m retrieved_docs)\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m context_text\n\u001b[1;32m     17\u001b[0m parallel_chain \u001b[38;5;241m=\u001b[39m RunnableParallel({\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mretriever\u001b[49m \u001b[38;5;241m|\u001b[39m RunnableLambda(format_docs),\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: RunnablePassthrough()\n\u001b[1;32m     20\u001b[0m })\n\u001b[1;32m     22\u001b[0m parallel_chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwho is Demis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     25\u001b[0m     template\u001b[38;5;241m=\u001b[39mtemplate,\n\u001b[1;32m     26\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#STEP6: CREATE PROMPT TEMPLATE\n",
    "template = \"\"\"\n",
    "Use the pieces of information provided in the context to answer user's question.\n",
    "If you dont know the answer, just say that you dont know, dont try to make up an answer. \n",
    "Dont provide anything out of the given context\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Start the answer directly. No small talk please.\n",
    "\"\"\"\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "  return context_text\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'context': retriever | RunnableLambda(format_docs),\n",
    "    'question': RunnablePassthrough()\n",
    "})\n",
    "\n",
    "parallel_chain.invoke('who is Demis')\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "main_chain = parallel_chain | prompt | llm | parser\n",
    "\n",
    "\n",
    "def chatbot():\n",
    "    query = input(\"Ask a question: \")\n",
    "    result = main_chain.invoke(query)\n",
    "    print(result)\n",
    "    \n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LANGCHAIN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
